{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49c0161",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (opcional) crear venv/conda\n",
        "pip install -U pandas numpy matplotlib scikit-learn catboost geopandas shap folium pyproj rtree fiona\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa49cfcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear carpetas de salida si no existen\n",
        "from pathlib import Path\n",
        "for p in [\"reports/figures\",\"reports/tables\"]:\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5117c9c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando variable de lluvia base para EDA y target: rfh\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "PATH_PARQUET = Path(\"data/processed/ven_rainfall_adm2.parquet\")\n",
        "PATH_MAP_BEST = Path(\"data/external/adm2_pcode_to_gid2_BEST.csv\")  # si existe\n",
        "assert PATH_PARQUET.exists(), \"No encuentro data/processed/ven_rainfall_adm2.parquet\"\n",
        "\n",
        "df = pd.read_parquet(PATH_PARQUET)\n",
        "\n",
        "# Normalizar columnas mínimas esperadas\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "if \"date\" in df.columns:\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "elif \"Date\" in df.columns:\n",
        "    df[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "else:\n",
        "    raise ValueError(\"No encuentro columna de fecha ('date' o 'Date').\")\n",
        "\n",
        "# Asegurar ADM2_PCODE\n",
        "if \"ADM2_PCODE\" not in df.columns:\n",
        "    raise ValueError(\"Falta columna ADM2_PCODE en el parquet.\")\n",
        "\n",
        "# Variables candidatas presentes\n",
        "cands_rain = [c for c in [\"rfh\",\"r1h\",\"r3h\"] if c in df.columns]\n",
        "cands_pct  = [c for c in [\"r1q\",\"r3q\",\"rfq\"] if c in df.columns]\n",
        "assert len(cands_rain)>0, \"No encuentro columnas de lluvia (rfh/r1h/r3h).\"\n",
        "\n",
        "# Por defecto trabajaremos con 'rfh' si existe; si no, con la primera disponible.\n",
        "y_base = \"rfh\" if \"rfh\" in cands_rain else cands_rain[0]\n",
        "print(\"Usando variable de lluvia base para EDA y target:\", y_base)\n",
        "\n",
        "# (Opcional) cargar mapping ADM2→Estado/Municipio si está disponible\n",
        "map_df = None\n",
        "if PATH_MAP_BEST.exists():\n",
        "    map_df = pd.read_csv(PATH_MAP_BEST)\n",
        "    # Intenta detectar nombres de columnas de estado/municipio\n",
        "    # (ajusta estos alias si tus nombres reales difieren)\n",
        "    aliases_estado = [c for c in map_df.columns if c.lower() in {\"estado\",\"state\",\"adm1_name\"}]\n",
        "    aliases_muni   = [c for c in map_df.columns if c.lower() in {\"municipio\",\"municipality\",\"adm2_name\"}]\n",
        "    col_estado = aliases_estado[0] if aliases_estado else None\n",
        "    col_muni   = aliases_muni[0]   if aliases_muni   else None\n",
        "    print(\"Mapping cargado. Estado:\", col_estado, \"| Municipio:\", col_muni)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b0510c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "figpath = Path(\"reports/figures\")\n",
        "\n",
        "ts = (df\n",
        "      .dropna(subset=[\"date\", y_base])\n",
        "      .groupby(\"date\", as_index=False)[y_base].mean()\n",
        "      .sort_values(\"date\"))\n",
        "ts[\"rolling\"] = ts[y_base].rolling(6, min_periods=1).mean()  # media móvil ~60 días\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(ts[\"date\"], ts[y_base], label=\"Media decádica nacional\")\n",
        "plt.plot(ts[\"date\"], ts[\"rolling\"], linewidth=2, label=\"Media móvil (6 decádicas)\")\n",
        "plt.title(f\"Evolución decádica nacional de precipitación ({y_base})\")\n",
        "plt.xlabel(\"Fecha\"); plt.ylabel(\"mm/decádica\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(figpath/\"eda_serie_nacional.png\", dpi=200)\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab995d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "if map_df is not None and col_estado is not None:\n",
        "    df_eda = df.copy()\n",
        "    df_eda = df_eda.merge(map_df[[\"ADM2_PCODE\", col_estado]], on=\"ADM2_PCODE\", how=\"left\")\n",
        "    last10 = df_eda[\"date\"] >= (df_eda[\"date\"].max() - pd.DateOffset(years=10))\n",
        "    bx = df_eda.loc[last10 & df_eda[y_base].notna(), [col_estado, y_base]].dropna()\n",
        "    top_estados = (bx.groupby(col_estado)[y_base].count()\n",
        "                     .sort_values(ascending=False).head(16).index.tolist())\n",
        "    bx = bx[bx[col_estado].isin(top_estados)]\n",
        "    # Boxplot con matplotlib puro\n",
        "    data = [bx.loc[bx[col_estado]==st, y_base].values for st in top_estados]\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.boxplot(data, labels=top_estados, vert=True, showfliers=False)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(f\"{y_base}: distribución por Estado (últimos 10 años)\")\n",
        "    plt.ylabel(\"mm/decádica\"); plt.tight_layout()\n",
        "    plt.savefig(figpath/\"eda_boxplot_estado_ult10y.png\", dpi=200)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b61858e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(cands_pct)>0:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    vals = pd.concat([df[c].dropna() for c in cands_pct], axis=0)\n",
        "    vals = vals[(vals>=0)&(vals<=100)]\n",
        "    plt.hist(vals, bins=20)\n",
        "    plt.title(\"Distribución de percentiles de precipitación (r1q/r3q/rfq)\")\n",
        "    plt.xlabel(\"Percentil [0-100]\"); plt.ylabel(\"Frecuencia\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath/\"eda_hist_percentiles.png\", dpi=200)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9454a349",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_14640\\4201890876.py:11: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: add_lags(g, lag_cols, lags=(1,2,7,14))))\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_14640\\4201890876.py:29: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(shift_target)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_14640\\4201890876.py:29: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(shift_target)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cortes temporales -> t+1: 2016-08-21 | t+3: 2016-08-03\n"
          ]
        }
      ],
      "source": [
        "def add_lags(g, cols, lags=(1,2,7,14)):\n",
        "    g = g.sort_values(\"date\").copy()\n",
        "    for col in cols:\n",
        "        for L in lags:\n",
        "            g[f\"{col}_lag{L}\"] = g[col].shift(L)\n",
        "    return g\n",
        "\n",
        "# Construimos features de lags para variables disponibles\n",
        "lag_cols = list(set(cands_rain + cands_pct))\n",
        "df_lag = (df.groupby(\"ADM2_PCODE\", group_keys=False)\n",
        "            .apply(lambda g: add_lags(g, lag_cols, lags=(1,2,7,14))))\n",
        "\n",
        "# Partes temporales\n",
        "df_lag[\"month\"] = df_lag[\"date\"].dt.month\n",
        "df_lag[\"month_sin\"] = np.sin(2*np.pi*df_lag[\"month\"]/12)\n",
        "df_lag[\"month_cos\"] = np.cos(2*np.pi*df_lag[\"month\"]/12)\n",
        "\n",
        "# Selección de filas válidas (sin NaN en features por los lags iniciales)\n",
        "min_lag = 14\n",
        "df_lag = df_lag[df_lag[\"date\"] >= (df_lag[\"date\"].min() + pd.Timedelta(days=10*min_lag))].copy()\n",
        "\n",
        "# Construcción del objetivo desplazado por horizonte h (en decádicas)\n",
        "def make_supervised(data, y_col, horizon_decads=1):\n",
        "    def shift_target(g):\n",
        "        g = g.sort_values(\"date\").copy()\n",
        "        g[f\"{y_col}_tplus{horizon_decads}\"] = g[y_col].shift(-horizon_decads)\n",
        "        return g\n",
        "    out = (data.groupby(\"ADM2_PCODE\", group_keys=False)\n",
        "                 .apply(shift_target)\n",
        "                 .dropna(subset=[f\"{y_col}_tplus{horizon_decads}\"]))\n",
        "    return out\n",
        "\n",
        "ds_t1 = make_supervised(df_lag, y_base, horizon_decads=1)\n",
        "ds_t3 = make_supervised(df_lag, y_base, horizon_decads=3)\n",
        "\n",
        "# Definir features X y target y\n",
        "def get_Xy(ds, y_col, horizon_decads):\n",
        "    target = f\"{y_col}_tplus{horizon_decads}\"\n",
        "    # Features: todos los lags + month_sin/cos + month (evita usar el y_base sin lag!)\n",
        "    feats = [c for c in ds.columns if (\"_lag\" in c) or (c in [\"month\",\"month_sin\",\"month_cos\"])]\n",
        "    X = ds[feats].astype(\"float32\")\n",
        "    y = ds[target].astype(\"float32\")\n",
        "    return X, y, feats\n",
        "\n",
        "X1, y1, feats1 = get_Xy(ds_t1, y_base, 1)\n",
        "X3, y3, feats3 = get_Xy(ds_t3, y_base, 3)\n",
        "\n",
        "# Split temporal 80/20 por fecha\n",
        "def temporal_split(ds, frac=0.8):\n",
        "    cutoff = ds[\"date\"].quantile(frac)\n",
        "    train_idx = ds[\"date\"] <= cutoff\n",
        "    test_idx  = ds[\"date\"]  > cutoff\n",
        "    return train_idx, test_idx, pd.to_datetime(cutoff)\n",
        "\n",
        "tr1, te1, cutoff1 = temporal_split(ds_t1, 0.8)\n",
        "tr3, te3, cutoff3 = temporal_split(ds_t3, 0.8)\n",
        "\n",
        "print(\"Cortes temporales -> t+1:\", cutoff1.date(), \"| t+3:\", cutoff3.date())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c9adb9ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF t+1 -> MAE: 15.04 | RMSE: 22.07\n",
            "CatBoost t+1 -> MAE: 15.16 | RMSE: 22.17\n",
            "RF t+3 -> MAE: 15.82 | RMSE: 22.97\n",
            "CatBoost t+3 -> MAE: 15.69 | RMSE: 22.82\n",
            "     Modelo Horizonte        MAE       RMSE       Corte\n",
            "0        RF       t+1  15.044909  22.065927  2016-08-21\n",
            "1  CatBoost       t+1  15.161029  22.169002  2016-08-21\n",
            "2        RF       t+3  15.815195  22.965524  2016-08-03\n",
            "3  CatBoost       t+3  15.686285  22.824318  2016-08-03\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "def eval_model(model, X_train, y_train, X_test, y_test, label):\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "    rmse = sqrt(mean_squared_error(y_test, pred))\n",
        "    print(f\"{label} -> MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
        "    return mae, rmse\n",
        "\n",
        "rf = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"rf\", RandomForestRegressor(\n",
        "        n_estimators=180, max_depth=12, min_samples_leaf=2,\n",
        "        bootstrap=True, max_samples=0.6, n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "# CatBoost (tree boosting) sin one-hot y sin GPU requerido\n",
        "from catboost import CatBoostRegressor\n",
        "cb_params = dict(\n",
        "    depth=8, learning_rate=0.1, iterations=700,\n",
        "    loss_function=\"RMSE\", random_seed=42, verbose=False\n",
        ")\n",
        "cb_t1 = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                  (\"cb\", CatBoostRegressor(**cb_params))])\n",
        "cb_t3 = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                  (\"cb\", CatBoostRegressor(**cb_params))])\n",
        "\n",
        "# t+1\n",
        "mae_rf1, rmse_rf1 = eval_model(rf, X1[tr1], y1[tr1], X1[te1], y1[te1], \"RF t+1\")\n",
        "mae_cb1, rmse_cb1 = eval_model(cb_t1, X1[tr1], y1[tr1], X1[te1], y1[te1], \"CatBoost t+1\")\n",
        "\n",
        "# t+3\n",
        "mae_rf3, rmse_rf3 = eval_model(rf, X3[tr3], y3[tr3], X3[te3], y3[te3], \"RF t+3\")\n",
        "mae_cb3, rmse_cb3 = eval_model(cb_t3, X3[tr3], y3[tr3], X3[te3], y3[te3], \"CatBoost t+3\")\n",
        "\n",
        "# Guardar tabla comparativa\n",
        "cmp = pd.DataFrame({\n",
        "    \"Modelo\":[\"RF\",\"CatBoost\",\"RF\",\"CatBoost\"],\n",
        "    \"Horizonte\":[\"t+1\",\"t+1\",\"t+3\",\"t+3\"],\n",
        "    \"MAE\":[mae_rf1, mae_cb1, mae_rf3, mae_cb3],\n",
        "    \"RMSE\":[rmse_rf1, rmse_cb1, rmse_rf3, rmse_cb3],\n",
        "    \"Corte\":[cutoff1.date(), cutoff1.date(), cutoff3.date(), cutoff3.date()]\n",
        "})\n",
        "cmp.to_csv(\"reports/tables/model_compare.csv\", index=False)\n",
        "print(cmp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3e70bc05",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "subset_t1 = cmp[cmp[\"Horizonte\"]==\"t+1\"]\n",
        "subset_t3 = cmp[cmp[\"Horizonte\"]==\"t+3\"]\n",
        "\n",
        "ax[0].bar(subset_t1[\"Modelo\"], subset_t1[\"MAE\"])\n",
        "ax[0].set_title(\"MAE por modelo (t+1)\"); ax[0].set_ylabel(\"MAE (mm)\")\n",
        "ax[1].bar(subset_t3[\"Modelo\"], subset_t3[\"MAE\"])\n",
        "ax[1].set_title(\"MAE por modelo (t+3)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"reports/figures/compare_models_mae.png\", dpi=200); plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].bar(subset_t1[\"Modelo\"], subset_t1[\"RMSE\"])\n",
        "ax[0].set_title(\"RMSE por modelo (t+1)\"); ax[0].set_ylabel(\"RMSE (mm)\")\n",
        "ax[1].bar(subset_t3[\"Modelo\"], subset_t3[\"RMSE\"])\n",
        "ax[1].set_title(\"RMSE por modelo (t+3)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"reports/figures/compare_models_rmse.png\", dpi=200); plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "baf1c537",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reentrenar RF en t+1 para extraer importancias (sobre train completo)\n",
        "rf.fit(X1[tr1], y1[tr1])\n",
        "importances = rf.named_steps[\"rf\"].feature_importances_\n",
        "imp_df = pd.DataFrame({\"feature\": feats1, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "imp_df.head(20).to_csv(\"reports/tables/feature_importances_t1.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "topk = imp_df.head(15).iloc[::-1]  # invertimos para barra horizontal\n",
        "plt.barh(topk[\"feature\"], topk[\"importance\"])\n",
        "plt.title(\"Importancias de variables (RF, t+1)\"); plt.tight_layout()\n",
        "plt.savefig(\"reports/figures/feature_importances_t1.png\", dpi=200); plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6c1e8ac5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Climatología basada en: rfh\n",
            "OK -> GeoJSON enriquecido: data\\external\\ven_adm2_with_preds_ONEPERMUNI_ENRIQUECIDO.geojson\n",
            "OK -> CSV: reports/tables/alertas_por_estado.csv\n",
            "OK -> PNG: reports/figures/alertas_por_estado_stacked.png\n"
          ]
        }
      ],
      "source": [
        "# ====== IMPORTS ======\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ====== PATHS ======\n",
        "PATH_PARQUET = Path(\"data/processed/ven_rainfall_adm2.parquet\")   # histórico CHIRPS limpio\n",
        "PATH_GEO_IN  = Path(\"data/external/ven_adm2_with_preds_ONEPERMUNI.geojson\")  # tu GeoJSON con Pred_mm\n",
        "PATH_GEO_OUT = Path(\"data/external/ven_adm2_with_preds_ONEPERMUNI_ENRIQUECIDO.geojson\")\n",
        "\n",
        "Path(\"reports/figures\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"reports/tables\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "assert PATH_PARQUET.exists(), f\"No existe {PATH_PARQUET}\"\n",
        "assert PATH_GEO_IN.exists(), f\"No existe {PATH_GEO_IN}\"\n",
        "\n",
        "# ====== 1) CARGA ======\n",
        "df = pd.read_parquet(PATH_PARQUET)  # histórico\n",
        "gdf = gpd.read_file(PATH_GEO_IN)    # predicciones actuales\n",
        "\n",
        "# Normaliza nombres mínimos\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "gdf.columns = [c.strip() for c in gdf.columns]\n",
        "\n",
        "# Fecha en ambos\n",
        "if \"date\" in df.columns:\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "elif \"Date\" in df.columns:\n",
        "    df[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "else:\n",
        "    raise ValueError(\"No encuentro columna de fecha en el parquet (date/Date).\")\n",
        "\n",
        "if \"Fecha_pred\" not in gdf.columns:\n",
        "    raise ValueError(\"El GeoJSON no tiene 'Fecha_pred'. Añádela antes de enriquecer.\")\n",
        "gdf[\"Fecha_pred\"] = pd.to_datetime(gdf[\"Fecha_pred\"], errors=\"coerce\")\n",
        "\n",
        "# Comprobaciones mínimas\n",
        "for col in [\"ADM2_PCODE\"]:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Falta {col} en parquet.\")\n",
        "    if col not in gdf.columns:\n",
        "        raise ValueError(f\"Falta {col} en GeoJSON.\")\n",
        "\n",
        "# Variable de lluvia base para climatología (ajusta si quieres otra)\n",
        "# Usaremos rfh (mm por decádica) si existe; si no, r1h/r3h\n",
        "cands_rain = [c for c in [\"rfh\", \"r1h\", \"r3h\"] if c in df.columns]\n",
        "if not cands_rain:\n",
        "    raise ValueError(\"No encuentro columnas de lluvia (rfh/r1h/r3h) en el parquet.\")\n",
        "rain_col = \"rfh\" if \"rfh\" in cands_rain else cands_rain[0]\n",
        "print(\"Climatología basada en:\", rain_col)\n",
        "\n",
        "# ====== 2) CLIMATOLOGÍA (ADM2 x MES) ======\n",
        "df[\"month\"] = df[\"date\"].dt.month\n",
        "\n",
        "def safe_percentile(x, q):\n",
        "    # evita problemas con NaN y series cortas\n",
        "    x = x.dropna().values\n",
        "    if len(x) == 0:\n",
        "        return np.nan\n",
        "    return float(np.percentile(x, q))\n",
        "\n",
        "agg = (df\n",
        "       .groupby([\"ADM2_PCODE\", \"month\"], as_index=False)\n",
        "       .agg(\n",
        "           Media_hist_mm=(rain_col, \"mean\"),\n",
        "           P05=(rain_col, lambda s: safe_percentile(s, 5)),\n",
        "           P10=(rain_col, lambda s: safe_percentile(s, 10)),\n",
        "           P20=(rain_col, lambda s: safe_percentile(s, 20)),\n",
        "           P80=(rain_col, lambda s: safe_percentile(s, 80)),\n",
        "           P90=(rain_col, lambda s: safe_percentile(s, 90)),\n",
        "           P95=(rain_col, lambda s: safe_percentile(s, 95)),\n",
        "           Sigma=(rain_col, \"std\")\n",
        "       ))\n",
        "\n",
        "# ====== 3) FUSIÓN GEOJSON + CLIMATOLOGÍA POR MES ======\n",
        "gdf[\"month\"] = gdf[\"Fecha_pred\"].dt.month\n",
        "enriq = gdf.merge(agg, on=[\"ADM2_PCODE\",\"month\"], how=\"left\")\n",
        "\n",
        "# ====== 4) CALCULAR ÍNDICES Y ALERTA ======\n",
        "# Diff_pct\n",
        "enriq[\"Diff_pct\"] = np.where(\n",
        "    enriq[\"Media_hist_mm\"].gt(0),\n",
        "    100.0 * (enriq[\"Pred_mm\"] - enriq[\"Media_hist_mm\"]) / enriq[\"Media_hist_mm\"],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# SPI-like\n",
        "enriq[\"SPI_like\"] = np.where(\n",
        "    enriq[\"Sigma\"].fillna(0).abs() > 1e-9,\n",
        "    (enriq[\"Pred_mm\"] - enriq[\"Media_hist_mm\"]) / enriq[\"Sigma\"],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# Categorías por percentiles\n",
        "def clasificar_alerta(row):\n",
        "    p05, p10, p20 = row[\"P05\"], row[\"P10\"], row[\"P20\"]\n",
        "    p80, p90, p95 = row[\"P80\"], row[\"P90\"], row[\"P95\"]\n",
        "    pred = row[\"Pred_mm\"]\n",
        "\n",
        "    if pd.isna(pred) or pd.isna(p05) or pd.isna(p95):\n",
        "        return \"Sin dato\"\n",
        "\n",
        "    # Precedencia: extremos -> intensos/severos -> moderados -> normal\n",
        "    if pred <= p05:\n",
        "        return \"Sequía extrema\"\n",
        "    if pred >= p95:\n",
        "        return \"Lluvia extrema\"\n",
        "    if not pd.isna(p10) and pred <= p10:\n",
        "        return \"Sequía severa\"\n",
        "    if not pd.isna(p90) and pred >= p90:\n",
        "        return \"Lluvia intensa\"\n",
        "    if not pd.isna(p20) and not pd.isna(p80) and (pred <= p20 or pred >= p80):\n",
        "        # si cae fuera de [P20, P80] sin llegar a severa/intensa\n",
        "        # distinguimos por el lado, solo informativo (puedes poner \"Moderada\")\n",
        "        return \"Sequía moderada\" if pred <= p20 else \"Lluvia moderada\"\n",
        "    return \"Normal\"\n",
        "\n",
        "enriq[\"Alerta\"] = enriq.apply(clasificar_alerta, axis=1)\n",
        "\n",
        "# ====== 5) GUARDAR GEOJSON ENRIQUECIDO ======\n",
        "# Conserva la geometría y escribe las nuevas properties\n",
        "enriq_gdf = gpd.GeoDataFrame(enriq, geometry=\"geometry\", crs=gdf.crs)\n",
        "enriq_gdf.to_file(PATH_GEO_OUT, driver=\"GeoJSON\")\n",
        "print(f\"OK -> GeoJSON enriquecido: {PATH_GEO_OUT}\")\n",
        "\n",
        "# ====== 6) TABLA DE ALERTAS POR ESTADO + GRÁFICO ======\n",
        "# Detecta columna de Estado en el GeoJSON\n",
        "cols_lower = {c.lower(): c for c in enriq_gdf.columns}\n",
        "col_estado = None\n",
        "for c in [\"estado\", \"state\", \"adm1_name\", \"adm1\", \"name_1\"]:\n",
        "    if c in cols_lower: \n",
        "        col_estado = cols_lower[c]; break\n",
        "if col_estado is None:\n",
        "    # en tus columnas viene \"Estado\"\n",
        "    if \"Estado\" in enriq_gdf.columns:\n",
        "        col_estado = \"Estado\"\n",
        "    else:\n",
        "        raise ValueError(\"No encuentro columna de Estado en el GeoJSON.\")\n",
        "\n",
        "tabla = (\n",
        "    enriq_gdf[[col_estado, \"Alerta\"]]\n",
        "    .dropna()\n",
        "    .groupby([col_estado, \"Alerta\"]).size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "# Orden sugerido de columnas\n",
        "orden = [\"Sequía extrema\",\"Sequía severa\",\"Sequía moderada\",\"Normal\",\"Lluvia moderada\",\"Lluvia intensa\",\"Lluvia extrema\"]\n",
        "cols_presentes = [c for c in orden if c in tabla.columns]\n",
        "otras = [c for c in tabla.columns if c not in cols_presentes]\n",
        "tabla = tabla.reindex(columns=cols_presentes + otras, fill_value=0)\n",
        "\n",
        "tabla[\"Total\"] = tabla.sum(axis=1)\n",
        "tabla = tabla.sort_values(\"Total\", ascending=False)\n",
        "\n",
        "tabla.reset_index().rename(columns={col_estado:\"Estado\"}).to_csv(\n",
        "    \"reports/tables/alertas_por_estado.csv\", index=False, encoding=\"utf-8-sig\"\n",
        ")\n",
        "print(\"OK -> CSV: reports/tables/alertas_por_estado.csv\")\n",
        "\n",
        "# Gráfico apilado Top-12\n",
        "top_states = tabla.head(12).copy()\n",
        "cats = [c for c in top_states.columns if c != \"Total\"]\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "bottom = np.zeros(len(top_states))\n",
        "for cat in cats:\n",
        "    plt.bar(top_states.index.astype(str), top_states[cat].values, bottom=bottom, label=str(cat))\n",
        "    bottom += top_states[cat].values\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Alertas por Estado (top 12)\")\n",
        "plt.ylabel(\"Nº de municipios\")\n",
        "plt.legend(ncol=3, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"reports/figures/alertas_por_estado_stacked.png\", dpi=200)\n",
        "plt.close()\n",
        "print(\"OK -> PNG: reports/figures/alertas_por_estado_stacked.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3873feec",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_14640\\471503852.py:6: DtypeWarning: Columns (1,2,4,5,6,7,8,9,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_new = pd.read_csv(\"data/raw/ven_rainfall_latest.csv\")\n"
          ]
        }
      ],
      "source": [
        "import requests, pandas as pd\n",
        "url = \"https://data.humdata.org/dataset/c136cc6e-573e-43b3-b7db-6e7e9648fd60/resource/8ca18230-5d6c-477c-a5a5-6b9ceebc6a5f/download/ven-rainfall-subnat-full.csv\"\n",
        "r = requests.get(url)\n",
        "open(\"data/raw/ven_rainfall_latest.csv\",\"wb\").write(r.content)\n",
        "\n",
        "df_new = pd.read_csv(\"data/raw/ven_rainfall_latest.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Descargando CSV desde HDX (requests) ===\n",
            "Guardado: data\\raw\\ven_rainfall_hdx_20250910_082128.csv\n",
            "\n",
            "=== Actualizando parquet histórico ===\n",
            "[info] Renombrada clave 'PCODE' -> 'ADM2_PCODE'\n",
            "Parquet actualizado -> data\\processed\\ven_rainfall_adm2.parquet (filas: 572,804)\n",
            "\n",
            "=== Predicción t+1/t+3 ===\n",
            "\n",
            "=== Cargando modelos desde disco ===\n",
            "Guardado -> data/processed/predicciones.csv  (356 filas)\n",
            "\n",
            "=== Climatología mensual ===\n",
            "Guardado -> data/processed/predicciones_enriquecidas.csv  (356 filas)\n",
            "\n",
            "=== Construyendo GeoJSON con TODAS las geometrías (JSON/Pandas) ===\n",
            "[map-post] ✅ Pred_mm cubierto: 338/338\n",
            "[map] GeoJSON listo con todas las geometrías -> data\\external\\ven_adm2_with_preds.geojson\n",
            "[map] 🗺️  Mapa guardado en: reports\\figures\\mapa_predicciones.html\n",
            "\n",
            "=== Ejecución completa ✔ ===\n"
          ]
        }
      ],
      "source": [
        "# tfm_pipeline.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TFM – Predicción de precipitación y alertas por municipio (ADM2) en Venezuela\n",
        "- Descarga incremental desde HDX (CSV URL)\n",
        "- Parquet histórico deduplicado\n",
        "- RandomForest t+1 y t+3 con validación temporal\n",
        "- Predicciones + Climatología mensual + SPI-like + Alertas\n",
        "- Mapa Folium (JSON/Pandas) garantizando TODAS las geometrías del GADM base\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import re, json, argparse, warnings, glob\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ML\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import joblib\n",
        "\n",
        "# Folium (solo para render del HTML)\n",
        "try:\n",
        "    import folium\n",
        "    HAS_FOLIUM = True\n",
        "except Exception:\n",
        "    HAS_FOLIUM = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "CFG = {\n",
        "    \"source_url\": \"https://data.humdata.org/dataset/c136cc6e-573e-43b3-b7db-6e7e9648fd60/resource/8ca18230-5d6c-477c-a5a5-6b9ceebc6a5f/download/ven-rainfall-subnat-full.csv\",\n",
        "    \"dirs\": {\n",
        "        \"raw\":       \"data/raw\",\n",
        "        \"processed\": \"data/processed\",\n",
        "        \"external\":  \"data/external\",\n",
        "        \"models\":    \"models\",\n",
        "        \"reports\":   \"reports\",\n",
        "        \"figures\":   \"reports/figures\",\n",
        "        \"tables\":    \"reports/tables\",\n",
        "    },\n",
        "    \"files\": {\n",
        "        \"parquet\":        \"data/processed/ven_rainfall_adm2.parquet\",\n",
        "        \"predictions\":    \"data/processed/predicciones.csv\",\n",
        "        \"preds_with_meta\":\"data/processed/predicciones_enriquecidas.csv\",\n",
        "        \"model_t1\":       \"models/rf_t+1.joblib\",\n",
        "        \"model_t3\":       \"models/rf_t+3.joblib\",\n",
        "\n",
        "        # Geo base (GADM) y mapping opcional\n",
        "        \"gadm_json\": \"data/external/gadm41_VEN_2.json\",\n",
        "        \"mapping_csv\": \"data/external/adm2_pcode_to_gid2.csv\",\n",
        "\n",
        "        # RAW opcional para medias históricas\n",
        "        \"raw_hist_csv\": \"data/raw/ven-rainfall-adm2-full.csv\",\n",
        "\n",
        "        # GeoJSON de salida (siempre con TODAS las geometrías)\n",
        "        \"geojson_out\": \"data/external/ven_adm2_with_preds.geojson\",\n",
        "\n",
        "        # Mapa Folium final\n",
        "        \"map_html\":\"reports/figures/mapa_predicciones.html\",\n",
        "    },\n",
        "    \"random_state\": 42,\n",
        "    \"train_subsample\": 250_000,\n",
        "}\n",
        "\n",
        "FEATURE_BASES = [\"rfh\",\"rfh_avg\",\"r1h\",\"r1h_avg\",\"r3h\",\"r3h_avg\",\"rfq\",\"r1q\",\"r3q\"]\n",
        "LAGS = [1, 2, 7, 14]\n",
        "CALENDAR_COLS = [\"month\",\"doy\",\"year\"]\n",
        "\n",
        "# =========================\n",
        "# UTILIDADES\n",
        "# =========================\n",
        "def ensure_dirs():\n",
        "    for d in CFG[\"dirs\"].values():\n",
        "        Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _print_h(msg: str):\n",
        "    print(\"\\n=== \" + msg + \" ===\")\n",
        "\n",
        "def normalize_code(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    s = str(x).strip().upper()\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def normalize_name(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    s = str(s).strip().upper()\n",
        "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def coerce_num(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return pd.to_numeric(str(s).replace(\",\", \".\"), errors=\"coerce\")\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def _to_dt(s, utc: bool = False, fmt: str | None = None):\n",
        "    if fmt:\n",
        "        return pd.to_datetime(s, format=fmt, errors=\"coerce\", utc=utc)\n",
        "    return pd.to_datetime(s, errors=\"coerce\", utc=utc)\n",
        "\n",
        "def _rmse(y_true, y_pred) -> float:\n",
        "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
        "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "# =========================\n",
        "# INGESTA / ACTUALIZACIÓN\n",
        "# =========================\n",
        "def download_latest() -> Path:\n",
        "    ensure_dirs()\n",
        "    import requests\n",
        "    url = CFG[\"source_url\"]\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out = Path(CFG[\"dirs\"][\"raw\"]) / f\"ven_rainfall_hdx_{ts}.csv\"\n",
        "\n",
        "    _print_h(\"Descargando CSV desde HDX (requests)\")\n",
        "    headers = {\"User-Agent\": \"TFM-DS-UCM/1.0 (ingestion)\"}\n",
        "    r = requests.get(url, headers=headers, timeout=180)\n",
        "    r.raise_for_status()\n",
        "    out.write_bytes(r.content)\n",
        "    print(f\"Guardado: {out}\")\n",
        "    return out\n",
        "\n",
        "def update_parquet(raw_csv: Path) -> Path:\n",
        "    ensure_dirs()\n",
        "    pq = Path(CFG[\"files\"][\"parquet\"])\n",
        "\n",
        "    _print_h(\"Actualizando parquet histórico\")\n",
        "    df = pd.read_csv(raw_csv, low_memory=False)\n",
        "\n",
        "    # Fecha\n",
        "    if \"#date\" in df.columns:\n",
        "        df = df.rename(columns={\"#date\": \"date\"})\n",
        "    elif \"date\" not in df.columns:\n",
        "        date_alias = [c for c in df.columns if str(c).lower() in {\"date\",\"fecha\"}]\n",
        "        if date_alias:\n",
        "            df = df.rename(columns={date_alias[0]: \"date\"})\n",
        "        else:\n",
        "            raise ValueError(\"No encuentro columna de fecha en el CSV.\")\n",
        "    df[\"date\"] = _to_dt(df[\"date\"], fmt=\"%Y-%m-%d\")\n",
        "    df = df[df[\"date\"].notna()].copy()\n",
        "\n",
        "    # ADM2_PCODE\n",
        "    if \"ADM2_PCODE\" not in df.columns:\n",
        "        cand = [c for c in df.columns if \"ADM2\" in c.upper() and \"PCODE\" in c.upper()]\n",
        "        if cand:\n",
        "            df = df.rename(columns={cand[0]: \"ADM2_PCODE\"})\n",
        "            print(f\"[info] Renombrada clave '{cand[0]}' -> 'ADM2_PCODE'\")\n",
        "        elif \"PCODE\" in df.columns:\n",
        "            df = df.rename(columns={\"PCODE\": \"ADM2_PCODE\"})\n",
        "            print(\"[info] Renombrada clave 'PCODE' -> 'ADM2_PCODE'\")\n",
        "        else:\n",
        "            cand = [c for c in df.columns if str(c).lower() in {\"adm2_code\",\"adm2_pcod\",\"pcode\"}]\n",
        "            if cand:\n",
        "                df = df.rename(columns={cand[0]: \"ADM2_PCODE\"})\n",
        "                print(f\"[info] Renombrada clave '{cand[0]}' -> 'ADM2_PCODE'\")\n",
        "            else:\n",
        "                raise ValueError(\"Falta ADM2_PCODE en el CSV.\")\n",
        "    df[\"ADM2_PCODE\"] = df[\"ADM2_PCODE\"].apply(normalize_code)\n",
        "\n",
        "    # Númericas típicas\n",
        "    for c in FEATURE_BASES + [\"n_pixels\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].apply(coerce_num).astype(\"float32\")\n",
        "\n",
        "    if \"n_pixels\" in df.columns:\n",
        "        df = df[df[\"n_pixels\"] > 0].copy()\n",
        "\n",
        "    # Append incremental + deduplicado\n",
        "    if pq.exists():\n",
        "        old = pd.read_parquet(pq)\n",
        "        old[\"ADM2_PCODE\"] = old[\"ADM2_PCODE\"].apply(normalize_code)\n",
        "        old[\"date\"] = _to_dt(old[\"date\"])\n",
        "        all_df = pd.concat([old, df], ignore_index=True)\n",
        "    else:\n",
        "        all_df = df.copy()\n",
        "\n",
        "    all_df = (all_df.sort_values([\"ADM2_PCODE\",\"date\"])\n",
        "                    .drop_duplicates(subset=[\"ADM2_PCODE\",\"date\"], keep=\"last\"))\n",
        "\n",
        "    # Evitar problemas Arrow con object\n",
        "    for c in all_df.columns:\n",
        "        if all_df[c].dtype == \"object\":\n",
        "            all_df[c] = all_df[c].astype(str)\n",
        "\n",
        "    all_df.to_parquet(pq, index=False)\n",
        "    print(f\"Parquet actualizado -> {pq} (filas: {len(all_df):,})\")\n",
        "    return pq\n",
        "\n",
        "# =========================\n",
        "# FEATURES / MODELOS\n",
        "# =========================\n",
        "def _add_group_lags(g: pd.DataFrame, bases: list[str], lags: list[int]) -> pd.DataFrame:\n",
        "    g = g.sort_values(\"date\").copy()\n",
        "    for b in bases:\n",
        "        if b in g.columns:\n",
        "            for L in lags:\n",
        "                g[f\"{b}_lag{L}\"] = g[b].shift(L)\n",
        "    return g\n",
        "\n",
        "def _choose_rain_var(df: pd.DataFrame) -> str:\n",
        "    for v in [\"rfh\",\"rfh_avg\",\"r1h_avg\",\"r3h_avg\",\"r1h\",\"r3h\"]:\n",
        "        if v in df.columns:\n",
        "            return v\n",
        "    raise ValueError(\"No encuentro variable de lluvia.\")\n",
        "\n",
        "def _build_feature_table(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    gcols = [\"ADM2_PCODE\",\"date\"]\n",
        "    use_cols = [c for c in df.columns if c in set(FEATURE_BASES + gcols)]\n",
        "    base = df[use_cols].copy()\n",
        "    base[\"month\"] = base[\"date\"].dt.month.astype(\"int16\")\n",
        "    base[\"year\"]  = base[\"date\"].dt.year.astype(\"int16\")\n",
        "    base[\"doy\"]   = base[\"date\"].dt.dayofyear.astype(\"int16\")\n",
        "    base = (base.groupby(\"ADM2_PCODE\", group_keys=False)\n",
        "                  .apply(lambda g: _add_group_lags(g, FEATURE_BASES, LAGS)))\n",
        "    return base\n",
        "\n",
        "def _select_available_features(df: pd.DataFrame) -> list[str]:\n",
        "    feats = []\n",
        "    for b in FEATURE_BASES:\n",
        "        for L in LAGS:\n",
        "            col = f\"{b}_lag{L}\"\n",
        "            if col in df.columns:\n",
        "                feats.append(col)\n",
        "    for c in CALENDAR_COLS:\n",
        "        if c in df.columns:\n",
        "            feats.append(c)\n",
        "    return feats\n",
        "\n",
        "def _train_rf(X, y) -> Pipeline:\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=180, max_depth=12, min_samples_leaf=2,\n",
        "        bootstrap=True, max_samples=0.6, random_state=CFG[\"random_state\"], n_jobs=-1\n",
        "    )\n",
        "    pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                     (\"rf\", rf)])\n",
        "    pipe.fit(X, y)\n",
        "    return pipe\n",
        "\n",
        "def _temporal_split(df: pd.DataFrame, test_frac=0.2):\n",
        "    cutoff = df[\"date\"].quantile(1 - test_frac)\n",
        "    mask_tr = df[\"date\"] <= cutoff\n",
        "    mask_te = ~mask_tr\n",
        "    return mask_tr, mask_te, cutoff\n",
        "\n",
        "def _features_path(suffix: str) -> Path:\n",
        "    Path(CFG[\"dirs\"][\"models\"]).mkdir(parents=True, exist_ok=True)\n",
        "    return Path(CFG[\"dirs\"][\"models\"]) / f\"features_{suffix}.json\"\n",
        "\n",
        "def _save_feature_spec(suffix: str, feature_cols: list[str]) -> None:\n",
        "    with open(_features_path(suffix), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"features\": feature_cols}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def _load_feature_spec(suffix: str) -> list[str] | None:\n",
        "    p = _features_path(suffix)\n",
        "    if p.exists():\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f).get(\"features\")\n",
        "    return None\n",
        "\n",
        "def _align_features_for_inference(X: pd.DataFrame, expected_cols: list[str]) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    for c in expected_cols:\n",
        "        if c not in X.columns:\n",
        "            X[c] = np.nan\n",
        "    return X[expected_cols]\n",
        "\n",
        "def train_models(parquet_path: Path) -> tuple[Pipeline, Pipeline, list[str]]:\n",
        "    _print_h(\"Entrenamiento modelos t+1 y t+3\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    df[\"date\"] = _to_dt(df[\"date\"])\n",
        "    df = df.sort_values([\"ADM2_PCODE\",\"date\"]).reset_index(drop=True)\n",
        "\n",
        "    feat_df = _build_feature_table(df)\n",
        "    feature_cols = _select_available_features(feat_df)\n",
        "\n",
        "    y_base = _choose_rain_var(df)\n",
        "    feat_df[\"y_t1\"] = feat_df.groupby(\"ADM2_PCODE\")[y_base].shift(-1)\n",
        "    feat_df[\"y_t3\"] = feat_df.groupby(\"ADM2_PCODE\")[y_base].shift(-3)\n",
        "\n",
        "    cols = [\"ADM2_PCODE\",\"date\"] + feature_cols + [\"y_t1\",\"y_t3\"]\n",
        "    train_df = feat_df[cols].dropna(subset=feature_cols + [\"y_t1\"]).copy()\n",
        "\n",
        "    tr, te, cutoff = _temporal_split(train_df)\n",
        "\n",
        "    rs = CFG[\"random_state\"]\n",
        "    tr_idx = train_df[tr].index\n",
        "    if len(tr_idx) > CFG[\"train_subsample\"]:\n",
        "        tr_idx = np.random.RandomState(rs).choice(tr_idx, size=CFG[\"train_subsample\"], replace=False)\n",
        "\n",
        "    X_tr = train_df.loc[tr_idx, feature_cols].astype(\"float32\")\n",
        "    y_tr = train_df.loc[tr_idx, \"y_t1\"].astype(\"float32\")\n",
        "    X_te = train_df.loc[te, feature_cols].astype(\"float32\")\n",
        "    y_te = train_df.loc[te, \"y_t1\"].astype(\"float32\")\n",
        "\n",
        "    rf_t1 = _train_rf(X_tr, y_tr)\n",
        "    y_hat = rf_t1.predict(X_te)\n",
        "    mae = mean_absolute_error(y_te, y_hat)\n",
        "    rmse = _rmse(y_te, y_hat)\n",
        "    print(f\"[t+1] cutoff={cutoff.date()}  MAE={mae:.2f}  RMSE={rmse:.2f}  n_train={len(X_tr):,}  n_test={len(X_te):,}\")\n",
        "\n",
        "    mask3_tr = ~train_df.loc[tr_idx, \"y_t3\"].isna()\n",
        "    mask3_te = ~train_df.loc[te, \"y_t3\"].isna()\n",
        "    rf_t3 = _train_rf(X_tr[mask3_tr], train_df.loc[tr_idx, \"y_t3\"].astype(\"float32\")[mask3_tr])\n",
        "    y_hat3 = rf_t3.predict(X_te[mask3_te])\n",
        "    mae3 = mean_absolute_error(train_df.loc[te, \"y_t3\"][mask3_te], y_hat3)\n",
        "    rmse3 = _rmse(train_df.loc[te, \"y_t3\"][mask3_te], y_hat3)\n",
        "    print(f\"[t+3] cutoff={cutoff.date()}  MAE={mae3:.2f}  RMSE={rmse3:.2f}  n_train={mask3_tr.sum():,}  n_test={mask3_te.sum():,}\")\n",
        "\n",
        "    Path(CFG[\"dirs\"][\"models\"]).mkdir(parents=True, exist_ok=True)\n",
        "    joblib.dump(rf_t1, CFG[\"files\"][\"model_t1\"])\n",
        "    joblib.dump(rf_t3, CFG[\"files\"][\"model_t3\"])\n",
        "    _save_feature_spec(\"t1\", feature_cols)\n",
        "    _save_feature_spec(\"t3\", feature_cols)\n",
        "    return rf_t1, rf_t3, feature_cols\n",
        "\n",
        "def _load_or_train_models(parquet_path: Path, *, force_retrain: bool = False) -> tuple[Pipeline, Pipeline, list[str]]:\n",
        "    f1, f3 = Path(CFG[\"files\"][\"model_t1\"]), Path(CFG[\"files\"][\"model_t3\"])\n",
        "    spec_t1 = _load_feature_spec(\"t1\")\n",
        "    spec_t3 = _load_feature_spec(\"t3\")\n",
        "    have_models = f1.exists() and f3.exists()\n",
        "\n",
        "    if not have_models and not force_retrain:\n",
        "        raise FileNotFoundError(\n",
        "            \"No existen modelos entrenados en 'models/'. \"\n",
        "            \"Ejecuta primero con --retrain para entrenar y guardarlos.\"\n",
        "        )\n",
        "    if force_retrain or not have_models or (spec_t1 is None or spec_t3 is None):\n",
        "        return train_models(parquet_path)\n",
        "\n",
        "    _print_h(\"Cargando modelos desde disco\")\n",
        "    return joblib.load(f1), joblib.load(f3), (spec_t1 or spec_t3 or [])\n",
        "\n",
        "def predict_latest(parquet_path: Path, *, force_retrain: bool = False) -> pd.DataFrame:\n",
        "    _print_h(\"Predicción t+1/t+3\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    df[\"date\"] = _to_dt(df[\"date\"])\n",
        "    df = df.sort_values([\"ADM2_PCODE\",\"date\"]).reset_index(drop=True)\n",
        "\n",
        "    feat_df = _build_feature_table(df)\n",
        "    candidate_cols = _select_available_features(feat_df)\n",
        "\n",
        "    idx_last = feat_df.groupby(\"ADM2_PCODE\")[\"date\"].idxmax()\n",
        "    last = feat_df.loc[idx_last, [\"ADM2_PCODE\",\"date\"] + candidate_cols].copy()\n",
        "\n",
        "    rf_t1, rf_t3, spec_cols = _load_or_train_models(parquet_path, force_retrain=force_retrain)\n",
        "\n",
        "    if spec_cols:\n",
        "        X_last = _align_features_for_inference(last[candidate_cols], spec_cols).astype(\"float32\")\n",
        "    else:\n",
        "        X_last = last[candidate_cols].astype(\"float32\")\n",
        "\n",
        "    last[\"Pred_t1_mm\"] = rf_t1.predict(X_last).astype(\"float32\")\n",
        "    last[\"Pred_t3_mm\"] = np.nan\n",
        "    try:\n",
        "        last.loc[:, \"Pred_t3_mm\"] = rf_t3.predict(X_last).astype(\"float32\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] t+3 no pudo predecir en todos los ADM2: {e}\")\n",
        "\n",
        "    out = last.rename(columns={\"date\":\"Fecha_base\"}).copy()\n",
        "    out[\"Fecha_pred\"] = out[\"Fecha_base\"] + pd.Timedelta(days=10)\n",
        "    out.rename(columns={\"Pred_t1_mm\":\"Pred_mm\"}, inplace=True)\n",
        "    out = out[[\"ADM2_PCODE\",\"Fecha_pred\",\"Pred_mm\",\"Pred_t3_mm\",\"Fecha_base\"]]\n",
        "    out.to_csv(CFG[\"files\"][\"predictions\"], index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"Guardado -> {CFG['files']['predictions']}  ({len(out):,} filas)\")\n",
        "    return out\n",
        "\n",
        "# =========================\n",
        "# CLIMATOLOGÍA / ALERTAS\n",
        "# =========================\n",
        "def compute_climatology(parquet_path: Path) -> pd.DataFrame:\n",
        "    _print_h(\"Climatología mensual\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    df[\"date\"] = _to_dt(df[\"date\"])\n",
        "    df[\"month\"] = df[\"date\"].dt.month.astype(\"int16\")\n",
        "    var = _choose_rain_var(df)\n",
        "    tmp = df[[\"ADM2_PCODE\",\"month\",var]].dropna().copy()\n",
        "\n",
        "    def _agg(g):\n",
        "        x = g[var].astype(float)\n",
        "        return pd.Series({\n",
        "            \"Media_hist_mm\": x.mean(),\n",
        "            \"P05\": np.percentile(x, 5),\n",
        "            \"P10\": np.percentile(x,10),\n",
        "            \"P20\": np.percentile(x,20),\n",
        "            \"P80\": np.percentile(x,80),\n",
        "            \"P90\": np.percentile(x,90),\n",
        "            \"P95\": np.percentile(x,95),\n",
        "            \"Std\": x.std(ddof=0)\n",
        "        })\n",
        "\n",
        "    clim = tmp.groupby([\"ADM2_PCODE\",\"month\"], as_index=False).apply(_agg)\n",
        "    return clim.reset_index(drop=True)\n",
        "\n",
        "def enrich_predictions_with_climatology(pred_df: pd.DataFrame, clim_df: pd.DataFrame,\n",
        "                                        parquet_path: Path) -> pd.DataFrame:\n",
        "    pred = pred_df.copy()\n",
        "    pred[\"month\"] = pd.to_datetime(pred[\"Fecha_pred\"]).dt.month.astype(\"int16\")\n",
        "    enr = pred.merge(clim_df, on=[\"ADM2_PCODE\",\"month\"], how=\"left\")\n",
        "\n",
        "    enr[\"SPI_like\"] = np.where(\n",
        "        enr[\"Std\"].fillna(0) > 0,\n",
        "        (enr[\"Pred_mm\"] - enr[\"Media_hist_mm\"]) / enr[\"Std\"],\n",
        "        np.nan\n",
        "    )\n",
        "    enr[\"Diff_pct\"] = np.where(\n",
        "        enr[\"Media_hist_mm\"].fillna(0) > 0,\n",
        "        100.0 * (enr[\"Pred_mm\"] - enr[\"Media_hist_mm\"]) / enr[\"Media_hist_mm\"],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    hist = pd.read_parquet(parquet_path)\n",
        "    meta_cols = [c for c in [\"Estado\",\"Municipio\"] if c in hist.columns]\n",
        "    if meta_cols:\n",
        "        ids = (hist.sort_values(\"date\")\n",
        "                    .groupby(\"ADM2_PCODE\", as_index=False).tail(1)[[\"ADM2_PCODE\"] + meta_cols]\n",
        "                    .drop_duplicates(\"ADM2_PCODE\"))\n",
        "        enr = enr.merge(ids, on=\"ADM2_PCODE\", how=\"left\")\n",
        "\n",
        "    def _alerta(row):\n",
        "        v, p05, p10, p20, p80, p90, p95 = (\n",
        "            row[\"Pred_mm\"], row[\"P05\"], row[\"P10\"], row[\"P20\"], row[\"P80\"], row[\"P90\"], row[\"P95\"]\n",
        "        )\n",
        "        if pd.isna(v) or pd.isna(p05) or pd.isna(p95):\n",
        "            return \"Sin dato\"\n",
        "        if v >= p95: return \"Lluvia extrema\"\n",
        "        if v >= p90: return \"Lluvia intensa\"\n",
        "        if v >  p80: return \"Lluvia moderada\"\n",
        "        if v <= p05: return \"Sequía extrema\"\n",
        "        if v <= p10: return \"Sequía severa\"\n",
        "        if v <= p20: return \"Sequía moderada\"\n",
        "        return \"Normal\"\n",
        "\n",
        "    enr[\"Alerta\"] = enr.apply(_alerta, axis=1)\n",
        "\n",
        "    cols_out = [\"ADM2_PCODE\",\"Fecha_pred\",\"Pred_mm\",\"Media_hist_mm\",\"Diff_pct\",\"SPI_like\",\"Alerta\",\"month\",\"Pred_t3_mm\"]\n",
        "    cols_out += [c for c in [\"Estado\",\"Municipio\"] if c in enr.columns]\n",
        "    enr_out = enr[cols_out].copy()\n",
        "\n",
        "    Path(CFG[\"dirs\"][\"processed\"]).mkdir(parents=True, exist_ok=True)\n",
        "    enr_out.to_csv(CFG[\"files\"][\"preds_with_meta\"], index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"Guardado -> {CFG['files']['preds_with_meta']}  ({len(enr_out):,} filas)\")\n",
        "    return enr_out\n",
        "\n",
        "# =========================\n",
        "# MAPA (100% JSON/Pandas: garantiza TODAS las geometrías)\n",
        "# =========================\n",
        "def _gid2_state_id(gid2: str):\n",
        "    if not isinstance(gid2, str): return None\n",
        "    m = re.match(r\"^VEN\\.(\\d+)\\.\", gid2.upper().strip())\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _load_mapping() -> pd.DataFrame | None:\n",
        "    mp = Path(CFG[\"files\"][\"mapping_csv\"])\n",
        "    if not mp.exists():\n",
        "        return None\n",
        "    try:\n",
        "        m = pd.read_csv(mp)\n",
        "        m.columns = [c.strip().upper() for c in m.columns]\n",
        "        assert \"ADM2_PCODE\" in m.columns and \"GID_2\" in m.columns\n",
        "        m[\"ADM2_PCODE\"] = m[\"ADM2_PCODE\"].astype(str).str.strip().str.upper()\n",
        "        m[\"GID_2\"] = m[\"GID_2\"].astype(str).str.strip().str.upper()\n",
        "        return m.rename(columns={\"ADM2_PCODE\":\"ADM2_PCODE\", \"GID_2\":\"GID_2\"})\n",
        "    except Exception as e:\n",
        "        print(\"[map] Aviso: mapping inválido:\", e)\n",
        "        return None\n",
        "\n",
        "def _row_norm_names(p: dict) -> tuple[str,str]:\n",
        "    n1 = p.get(\"NAME_1\")\n",
        "    n2 = p.get(\"NAME_2\")\n",
        "    return normalize_name(n1), normalize_name(n2)\n",
        "\n",
        "def _find_pred_row(code: str, gid2: str, n1n: str, n2n: str, lut_code, lut_name, map_g2c):\n",
        "    # 1) Por ADM2_PCODE directo\n",
        "    if code and code in lut_code:\n",
        "        return lut_code[code]\n",
        "    # 2) Por GID_2 -> ADM2_PCODE\n",
        "    if gid2:\n",
        "        code2 = map_g2c.get(gid2)\n",
        "        if code2 and code2 in lut_code:\n",
        "            return lut_code[code2]\n",
        "    # 3) Por (NAME_1_n, NAME_2_n)\n",
        "    if n1n and n2n:\n",
        "        key = (n1n, n2n)\n",
        "        if key in lut_name:\n",
        "            return lut_name[key]\n",
        "    return None\n",
        "\n",
        "def build_map_geojson(enr_df: pd.DataFrame) -> Path | None:\n",
        "    \"\"\"\n",
        "    Ensambla SIEMPRE todas las geometrías del GADM base y les inyecta Pred_mm/Media/Diff_pct.\n",
        "    No usa GeoPandas; no depende de cobertura de mapping para incluir geometrías.\n",
        "    \"\"\"\n",
        "    _print_h(\"Construyendo GeoJSON con TODAS las geometrías (JSON/Pandas)\")\n",
        "\n",
        "    # 1) Cargar GADM base\n",
        "    gadm_path = Path(CFG[\"files\"][\"gadm_json\"])\n",
        "    if not gadm_path.exists():\n",
        "        print(f\"[map] No existe {gadm_path}.\")\n",
        "        return None\n",
        "    with open(gadm_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        fc = json.load(f)\n",
        "    feats = fc.get(\"features\", [])\n",
        "    if not feats:\n",
        "        print(\"[map] GeoJSON base sin features.\")\n",
        "        return None\n",
        "\n",
        "    # 2) Predicciones enriquecidas\n",
        "    preds_path = Path(CFG[\"files\"][\"preds_with_meta\"])\n",
        "    if not preds_path.exists():\n",
        "        print(f\"[map] No existe {preds_path}.\")\n",
        "        return None\n",
        "    preds = pd.read_csv(preds_path, low_memory=False)\n",
        "    # normaliza\n",
        "    preds[\"ADM2_PCODE\"] = preds[\"ADM2_PCODE\"].astype(str).str.strip().str.upper()\n",
        "    # nombres normalizados para join por nombre si estuvieran\n",
        "    for col_src, col_norm in [(\"Estado\",\"NAME_1_n\"), (\"Municipio\",\"NAME_2_n\")]:\n",
        "        if col_src in preds.columns and col_norm not in preds.columns:\n",
        "            preds[col_norm] = preds[col_src].apply(normalize_name)\n",
        "    if \"NAME_1_n\" not in preds.columns:\n",
        "        preds[\"NAME_1_n\"] = np.nan\n",
        "    if \"NAME_2_n\" not in preds.columns:\n",
        "        preds[\"NAME_2_n\"] = np.nan\n",
        "\n",
        "    # 3) Mapping opcional GID_2<->ADM2_PCODE\n",
        "    mapping = _load_mapping()\n",
        "    map_g2c = {}\n",
        "    if mapping is not None:\n",
        "        map_g2c = dict(zip(mapping[\"GID_2\"], mapping[\"ADM2_PCODE\"]))\n",
        "\n",
        "    # 4) Luts para búsqueda rápida\n",
        "    # por código\n",
        "    lut_code = {r.ADM2_PCODE: r for r in preds.itertuples(index=False)}\n",
        "    # por nombre\n",
        "    lut_name = {(r.NAME_1_n, r.NAME_2_n): r for r in preds.itertuples(index=False)\n",
        "                if isinstance(r.NAME_1_n, str) and isinstance(r.NAME_2_n, str)}\n",
        "\n",
        "    # 5) Recorremos TODAS las features y rellenamos properties\n",
        "    for feat in feats:\n",
        "        p = feat.get(\"properties\", {})\n",
        "        # claves base\n",
        "        gid2 = str(p.get(\"GID_2\",\"\") or \"\").strip().upper()\n",
        "        # si trae ADM2_PCODE en el GADM base, normalizarla\n",
        "        code = normalize_code(p.get(\"ADM2_PCODE\")) if p.get(\"ADM2_PCODE\") else None\n",
        "        n1n, n2n = _row_norm_names(p)\n",
        "\n",
        "        # si no hay ADM2_PCODE en el GADM, intenta via mapping por GID_2\n",
        "        if not code and gid2 and gid2 in map_g2c:\n",
        "            code = map_g2c[gid2]\n",
        "\n",
        "        # elegimos la fila de predicción\n",
        "        row = _find_pred_row(code, gid2, n1n, n2n, lut_code, lut_name, map_g2c)\n",
        "\n",
        "        # aseguramos ADM2_PCODE en properties (clave del choropleth)\n",
        "        if code:\n",
        "            p[\"ADM2_PCODE\"] = code\n",
        "        elif row is not None:\n",
        "            p[\"ADM2_PCODE\"] = row.ADM2_PCODE\n",
        "        else:\n",
        "            # última opción: sintetizar por nombres (igual al código que te funcionaba)\n",
        "            p[\"ADM2_PCODE\"] = f\"{n1n}|{n2n}\"\n",
        "\n",
        "        # métricas\n",
        "        if row is not None:\n",
        "            for k in [\"Fecha_pred\",\"Pred_mm\",\"Media_hist_mm\",\"Diff_pct\",\"SPI_like\",\"Alerta\"]:\n",
        "                if k in preds.columns:\n",
        "                    val = getattr(row, k, None)\n",
        "                    if pd.notna(val):\n",
        "                        if isinstance(val, (np.floating, float)):\n",
        "                            p[k] = float(val)\n",
        "                        else:\n",
        "                            p[k] = val\n",
        "        # deja NAME_1/NAME_2 (útil para tooltip)\n",
        "        if \"NAME_1\" in p and \"Estado\" not in p:\n",
        "            p[\"Estado\"] = p[\"NAME_1\"]\n",
        "        if \"NAME_2\" in p and \"Municipio\" not in p:\n",
        "            p[\"Municipio\"] = p[\"NAME_2\"]\n",
        "\n",
        "    # 6) DataFrame de props para completar medias/fills como en tu script\n",
        "    props_df = pd.DataFrame([f[\"properties\"] for f in feats])\n",
        "    # normalización\n",
        "    props_df[\"ADM2_PCODE\"] = props_df[\"ADM2_PCODE\"].astype(str).str.strip().str.upper()\n",
        "    props_df[\"Pred_mm\"] = pd.to_numeric(props_df.get(\"Pred_mm\", np.nan), errors=\"coerce\")\n",
        "    props_df[\"Media_hist_mm\"] = pd.to_numeric(props_df.get(\"Media_hist_mm\", np.nan), errors=\"coerce\")\n",
        "    props_df[\"GID_2\"] = props_df.get(\"GID_2\", np.nan)\n",
        "    props_df[\"STATE_ID\"] = props_df[\"GID_2\"].apply(_gid2_state_id)\n",
        "\n",
        "    # 6.a) Completar Media_hist_mm con RAW opcional\n",
        "    raw_path = Path(CFG[\"files\"][\"raw_hist_csv\"])\n",
        "    if raw_path.exists():\n",
        "        try:\n",
        "            h = pd.read_csv(raw_path, usecols=[\"ADM2_PCODE\",\"r1q\"], low_memory=False)\n",
        "            h[\"ADM2_PCODE\"] = h[\"ADM2_PCODE\"].astype(str).str.strip().str.upper()\n",
        "            h[\"r1q\"] = pd.to_numeric(h[\"r1q\"], errors=\"coerce\")\n",
        "            hist_mean = h.groupby(\"ADM2_PCODE\", as_index=False)[\"r1q\"].mean().rename(columns={\"r1q\":\"Media_hist_mm_from_hist\"})\n",
        "            props_df = props_df.merge(hist_mean, on=\"ADM2_PCODE\", how=\"left\")\n",
        "            props_df[\"Media_hist_mm\"] = np.where(props_df[\"Media_hist_mm\"].notna(),\n",
        "                                                 props_df[\"Media_hist_mm\"], props_df[\"Media_hist_mm_from_hist\"])\n",
        "            props_df.drop(columns=[c for c in props_df.columns if c.endswith(\"_from_hist\")], inplace=True)\n",
        "        except Exception as e:\n",
        "            print(\"[map-post] Aviso RAW->media falló:\", e)\n",
        "\n",
        "    # 6.b) FILL A: usar Media_hist_mm cuando falte Pred_mm\n",
        "    mask = props_df[\"Pred_mm\"].isna() & props_df[\"Media_hist_mm\"].notna()\n",
        "    props_df.loc[mask, \"Pred_mm\"] = props_df.loc[mask, \"Media_hist_mm\"]\n",
        "\n",
        "    # 6.c) FILL B: mediana por estado\n",
        "    state_med = props_df.groupby(\"STATE_ID\")[\"Pred_mm\"].median()\n",
        "    mask = props_df[\"Pred_mm\"].isna() & props_df[\"STATE_ID\"].notna()\n",
        "    props_df.loc[mask, \"Pred_mm\"] = props_df.loc[mask, \"STATE_ID\"].map(state_med)\n",
        "\n",
        "    # 6.d) FILL C: mediana nacional\n",
        "    nat_med = float(props_df[\"Pred_mm\"].median(skipna=True)) if props_df[\"Pred_mm\"].notna().any() else 0.0\n",
        "    props_df[\"Pred_mm\"] = props_df[\"Pred_mm\"].fillna(nat_med)\n",
        "\n",
        "    # 6.e) Diff_pct\n",
        "    props_df[\"Diff_pct\"] = np.where(\n",
        "        props_df[\"Media_hist_mm\"].notna() & props_df[\"Pred_mm\"].notna() & (props_df[\"Media_hist_mm\"]!=0),\n",
        "        100.0*(props_df[\"Pred_mm\"] - props_df[\"Media_hist_mm\"]) / props_df[\"Media_hist_mm\"],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    pred_ok = int(props_df[\"Pred_mm\"].notna().sum())\n",
        "    print(f\"[map-post] ✅ Pred_mm cubierto: {pred_ok}/{len(props_df)}\")\n",
        "\n",
        "    # 7) Escribir de vuelta en el GeoJSON\n",
        "    lut = props_df.set_index(\"ADM2_PCODE\")\n",
        "    for feat in feats:\n",
        "        p = feat[\"properties\"]\n",
        "        code = str(p.get(\"ADM2_PCODE\",\"\")).strip().upper()\n",
        "        if code in lut.index:\n",
        "            row = lut.loc[code]\n",
        "            # escribe métricas\n",
        "            p[\"Pred_mm\"] = float(row[\"Pred_mm\"]) if pd.notna(row[\"Pred_mm\"]) else None\n",
        "            p[\"Media_hist_mm\"] = float(row[\"Media_hist_mm\"]) if pd.notna(row[\"Media_hist_mm\"]) else None\n",
        "            p[\"Diff_pct\"] = float(row[\"Diff_pct\"]) if pd.notna(row[\"Diff_pct\"]) else None\n",
        "\n",
        "    out_geo = Path(CFG[\"files\"][\"geojson_out\"])\n",
        "    out_geo.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_geo, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fc, f, ensure_ascii=False)\n",
        "    print(f\"[map] GeoJSON listo con todas las geometrías -> {out_geo}\")\n",
        "\n",
        "    # 8) Render Folium desde ese mismo GeoJSON (choropleth por ADM2_PCODE)\n",
        "    if HAS_FOLIUM:\n",
        "        try:\n",
        "            rows = [{\"ADM2_PCODE\": r[\"ADM2_PCODE\"], \"Pred_mm\": r[\"Pred_mm\"]} for r in props_df.to_dict(\"records\")]\n",
        "            vals = pd.DataFrame(rows)\n",
        "            vals[\"ADM2_PCODE\"] = vals[\"ADM2_PCODE\"].astype(str).str.strip().str.upper()\n",
        "            vals[\"Pred_mm\"] = pd.to_numeric(vals[\"Pred_mm\"], errors=\"coerce\")\n",
        "\n",
        "            m = folium.Map(location=[7.5, -66.0], zoom_start=5, tiles=\"cartodbpositron\")\n",
        "\n",
        "            folium.Choropleth(\n",
        "                geo_data=str(out_geo),\n",
        "                name=\"Predicción (mm)\",\n",
        "                data=vals,\n",
        "                columns=[\"ADM2_PCODE\", \"Pred_mm\"],\n",
        "                key_on=\"feature.properties.ADM2_PCODE\",\n",
        "                fill_opacity=0.7,\n",
        "                line_opacity=0.2,\n",
        "                legend_name=\"Predicción (mm)\"\n",
        "            ).add_to(m)\n",
        "\n",
        "            # Tooltip robusto: usa Estado/Municipio si están, si no NAME_1/NAME_2\n",
        "            with open(out_geo, \"r\", encoding=\"utf-8\") as f2:\n",
        "                fc2 = json.load(f2)\n",
        "            available = set(fc2[\"features\"][0][\"properties\"].keys())\n",
        "            campo_estado    = \"Estado\"    if \"Estado\"    in available else (\"NAME_1\" if \"NAME_1\" in available else None)\n",
        "            campo_municipio = \"Municipio\" if \"Municipio\" in available else (\"NAME_2\" if \"NAME_2\" in available else None)\n",
        "            tooltip_fields = [c for c in [campo_estado, campo_municipio, \"ADM2_PCODE\", \"Fecha_pred\", \"Pred_mm\", \"Media_hist_mm\", \"Diff_pct\"] if c and c in available]\n",
        "            aliases = []\n",
        "            for f in tooltip_fields:\n",
        "                if f in (\"Estado\",\"NAME_1\"): aliases.append(\"Estado\")\n",
        "                elif f in (\"Municipio\",\"NAME_2\"): aliases.append(\"Municipio\")\n",
        "                elif f==\"ADM2_PCODE\": aliases.append(\"Código\")\n",
        "                elif f==\"Fecha_pred\": aliases.append(\"Fecha pred.\")\n",
        "                elif f==\"Pred_mm\": aliases.append(\"Predicción (mm)\")\n",
        "                elif f==\"Media_hist_mm\": aliases.append(\"Media hist. (mm)\")\n",
        "                elif f==\"Diff_pct\": aliases.append(\"Dif. vs media (%)\")\n",
        "                else: aliases.append(f)\n",
        "\n",
        "            folium.GeoJson(\n",
        "                str(out_geo),\n",
        "                name=\"Detalle\",\n",
        "                tooltip=folium.GeoJsonTooltip(fields=tooltip_fields, aliases=aliases, localize=True, sticky=True),\n",
        "                style_function=lambda x: {\"weight\": 0.2, \"color\": \"black\", \"fillOpacity\": 0.0}\n",
        "            ).add_to(m)\n",
        "\n",
        "            folium.LayerControl().add_to(m)\n",
        "            out_html = Path(CFG[\"files\"][\"map_html\"])\n",
        "            out_html.parent.mkdir(parents=True, exist_ok=True)\n",
        "            m.save(out_html.as_posix())\n",
        "            print(f\"[map] 🗺️  Mapa guardado en: {out_html}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[map] Advertencia al renderizar mapa: {e}\")\n",
        "    else:\n",
        "        print(\"[map] Folium no disponible: se omite render HTML.\")\n",
        "\n",
        "    return out_geo\n",
        "\n",
        "# =========================\n",
        "# RESÚMENES\n",
        "# =========================\n",
        "def export_state_summary(enr_df: pd.DataFrame):\n",
        "    Path(CFG[\"dirs\"][\"tables\"]).mkdir(parents=True, exist_ok=True)\n",
        "    if \"Estado\" not in enr_df.columns:\n",
        "        return\n",
        "    tab = (enr_df[[\"Estado\",\"Alerta\"]]\n",
        "           .dropna()\n",
        "           .groupby([\"Estado\",\"Alerta\"]).size()\n",
        "           .unstack(fill_value=0)\n",
        "           .sort_index())\n",
        "    tab[\"Total\"] = tab.sum(axis=1)\n",
        "    tab = tab.sort_values(\"Total\", ascending=False)\n",
        "    out = Path(CFG[\"dirs\"][\"tables\"]) / \"alertas_por_estado.csv\"\n",
        "    tab.to_csv(out, encoding=\"utf-8-sig\")\n",
        "    print(f\"Resumen por Estado -> {out}\")\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "def run(download: bool, predict: bool, build_map: bool, *, force_retrain: bool = False):\n",
        "    ensure_dirs()\n",
        "    pq_final = Path(CFG[\"files\"][\"parquet\"])\n",
        "\n",
        "    # 1) Descarga + actualización parquet\n",
        "    if download:\n",
        "        raw = download_latest()\n",
        "        pq_final = update_parquet(raw)\n",
        "    else:\n",
        "        if not pq_final.exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"No hay parquet histórico en {pq_final}. Ejecuta con --download la primera vez.\"\n",
        "            )\n",
        "\n",
        "    # 2) Predicción\n",
        "    if predict:\n",
        "        preds_df = predict_latest(pq_final, force_retrain=force_retrain)\n",
        "    else:\n",
        "        if Path(CFG[\"files\"][\"predictions\"]).exists():\n",
        "            preds_df = pd.read_csv(\n",
        "                CFG[\"files\"][\"predictions\"],\n",
        "                parse_dates=[\"Fecha_pred\",\"Fecha_base\"],\n",
        "                dayfirst=False, encoding=\"utf-8-sig\"\n",
        "            )\n",
        "        else:\n",
        "            preds_df = predict_latest(pq_final, force_retrain=force_retrain)\n",
        "\n",
        "    # 3) Climatología + alertas\n",
        "    clim_df = compute_climatology(pq_final)\n",
        "    enr_df = enrich_predictions_with_climatology(preds_df, clim_df, pq_final)\n",
        "    export_state_summary(enr_df)\n",
        "\n",
        "    # 4) Mapa (si se pide)\n",
        "    if build_map:\n",
        "        build_map_geojson(enr_df)\n",
        "\n",
        "    _print_h(\"Ejecución completa ✔\")\n",
        "\n",
        "# =========================\n",
        "# CLI\n",
        "# =========================\n",
        "def parse_args(argv=None):\n",
        "    p = argparse.ArgumentParser(\n",
        "        description=\"TFM – Pipeline precipitación Venezuela (HDX -> parquet -> RF -> predicciones -> alertas -> mapa)\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "    )\n",
        "    p.add_argument(\"--download\", action=\"store_true\", help=\"Descargar y actualizar parquet histórico\")\n",
        "    p.add_argument(\"--predict\",  action=\"store_true\", help=\"Generar/actualizar predicciones\")\n",
        "    p.add_argument(\"--map\",      action=\"store_true\", help=\"Construir GeoJSON y mapa Folium\")\n",
        "    p.add_argument(\"--retrain\",  action=\"store_true\", help=\"Entrenar/actualizar modelos (no automático)\")\n",
        "    if argv is None:\n",
        "        args, _ = p.parse_known_args()\n",
        "    else:\n",
        "        args = p.parse_args(argv)\n",
        "    return args\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    do_download = args.download or (not any([args.download, args.predict, args.map]))\n",
        "    do_predict  = args.predict  or (not any([args.download, args.predict, args.map]))\n",
        "    do_map      = args.map      or (not any([args.download, args.predict, args.map]))\n",
        "    run(download=do_download, predict=do_predict, build_map=do_map, force_retrain=args.retrain)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
